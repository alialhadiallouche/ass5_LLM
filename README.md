# ğŸ¥ Multimodal Video Question Answering (QA) System

This project implements a **Video QA system** that allows users to ask natural language questions and retrieve the most relevant video segment based on semantic and lexical information. It supports multiple retrieval methods (FAISS, TF-IDF, BM25), visual indexing, and a Streamlit-based UI.

---

## ğŸ“Œ Features

- ğŸ”Š Transcription using OpenAI Whisper
- ğŸ“– Re-chunked transcript into ~10 second blocks
- ğŸï¸ Keyframe extraction from video
- ğŸ§  Dense embeddings using `e5-large-v2` and OpenCLIP
- âš¡ Fast retrieval using FAISS
- ğŸ“Š Evaluation with a gold test set (answerable + unanswerable questions)
- ğŸŒ Clean Streamlit interface for interactive search
- ğŸ§ª Quantitative evaluation across FAISS, TF-IDF, BM25

---

## ğŸ”§ Setup Instructions
copy the github link and paste it in streamlit cloud


for Preparing Data:
The following steps are handled in Ass5_ana84.ipynb:

Transcribe video with Whisper

Extract frames with OpenCV

Generate chunked_transcript.json and frame_text_pairs.json

Encode text and images with sentence transformers and OpenCLIP

Build FAISS indices

Evaluation



ğŸ§ª Gold Test Set
Includes:

10 answerable questions with ground truth timestamps

5 unanswerable questions to evaluate rejection ability
